{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"cellId":"qxtsnrzkmzox9zzuko3rfi","id":"3679658c","outputId":"fcd9b0e6-8551-4582-a208-6e273fce3ed0","execution":{"iopub.status.busy":"2022-05-29T23:59:05.100997Z","iopub.execute_input":"2022-05-29T23:59:05.101421Z","iopub.status.idle":"2022-05-29T23:59:16.025836Z","shell.execute_reply.started":"2022-05-29T23:59:05.101329Z","shell.execute_reply":"2022-05-29T23:59:16.024868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install razdel","metadata":{"cellId":"uptflhp56nit1lysgllto","id":"f50ee828","outputId":"bfa0abc9-9018-4bdf-9512-3868abd819f0","execution":{"iopub.status.busy":"2022-05-29T23:59:16.029073Z","iopub.execute_input":"2022-05-29T23:59:16.029575Z","iopub.status.idle":"2022-05-29T23:59:25.66007Z","shell.execute_reply.started":"2022-05-29T23:59:16.02954Z","shell.execute_reply":"2022-05-29T23:59:25.659096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nfrom keras.layers import Input, Dense, Lambda, Layer\nfrom keras.layers.normalization import batch_normalization\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import metrics\nimport tensorflow as tf\nimport transformers\nfrom transformers import AutoTokenizer, BertTokenizer, TFBertModel\nimport time\nimport unicodedata as ud\nfrom tqdm.auto import tqdm\nfrom razdel import tokenize, sentenize\nfrom typing import List, Tuple, Union, Dict\nimport unicodedata as ud\nimport warnings\nimport copy\nimport codecs\nimport json\nimport random\nimport pickle","metadata":{"cellId":"lgx0zfhjciyhnvgijhnm","id":"b095eec6","execution":{"iopub.status.busy":"2022-05-29T23:59:25.671219Z","iopub.execute_input":"2022-05-29T23:59:25.672005Z","iopub.status.idle":"2022-05-29T23:59:34.045788Z","shell.execute_reply.started":"2022-05-29T23:59:25.671947Z","shell.execute_reply":"2022-05-29T23:59:34.044964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"cellId":"1xjtndgisln05bfmtl3ibi7","id":"615eae9a","outputId":"2d6d5f1f-a146-4e72-8fee-6ffc43fef1c1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2\n!wget https://github.com/dialogue-evaluation/RuNNE/blob/main/public_data/train.jsonl?raw=true\n!wget https://github.com/dialogue-evaluation/RuNNE/blob/main/public_data/ners.txt?raw=true","metadata":{"cellId":"pidk1g6oj567c56k1jxye","id":"8a7fa3ad","outputId":"b99afc59-f78b-446a-b2f2-937d69254e81","execution":{"iopub.status.busy":"2022-05-30T00:00:21.327171Z","iopub.execute_input":"2022-05-30T00:00:21.327911Z","iopub.status.idle":"2022-05-30T00:00:26.076385Z","shell.execute_reply.started":"2022-05-30T00:00:21.327866Z","shell.execute_reply":"2022-05-30T00:00:26.07527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Функция для парсинга строки json RuNNE","metadata":{"cellId":"4lsy40126edong3oq0hu8c","id":"62f0fefa"}},{"cell_type":"code","source":"#!c1.32\ndef parse_runne_json_string(s: str) -> Tuple[int, str, List[Tuple[str, int, int]]]:\n    data = json.loads(s)\n    if 'id' not in data:\n        err_msg = f'The \"id\" key is not found in the string \"{s}\".'\n        raise ValueError(err_msg)\n    if 'sentences' not in data:\n        err_msg = f'The \"sentences\" key is not found in the string \"{s}\".'\n        raise ValueError(err_msg)\n    identifier = data['id']\n    text = data['sentences']\n    ners = []\n    if 'ners' in data:\n        if len(text.strip()) == 0:\n            err_msg = f'The named entities are specified incorrectly ' \\\n                      f'in the string \"{s}\".'\n            raise ValueError(err_msg)\n        for idx, named_entity_info in enumerate(data['ners']):\n            if not isinstance(named_entity_info, list):\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            if len(named_entity_info) != 3:\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            if not isinstance(named_entity_info[0], int):\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            if not isinstance(named_entity_info[1], int):\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            if not isinstance(named_entity_info[2], str):\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            if named_entity_info[0] > named_entity_info[1]:\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            if named_entity_info[0] < 0:\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            if named_entity_info[1] >= len(text):\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            start_pos = named_entity_info[0]\n            end_pos = named_entity_info[1] + 1\n            if text[start_pos].isspace():\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            if text[end_pos - 1].isspace():\n                err_msg = f'Named entity {idx} is specified incorrectly ' \\\n                          f'in the string \"{s}\".'\n                raise ValueError(err_msg)\n            ners.append((named_entity_info[2], start_pos, end_pos))\n    return identifier, text, ners","metadata":{"cellId":"kdos8f3ad8l6yoe08prya","id":"62764e39","execution":{"iopub.status.busy":"2022-05-30T00:00:29.080191Z","iopub.execute_input":"2022-05-30T00:00:29.080783Z","iopub.status.idle":"2022-05-30T00:00:29.097675Z","shell.execute_reply.started":"2022-05-30T00:00:29.08074Z","shell.execute_reply":"2022-05-30T00:00:29.096866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef load_runne_data(fname: str) -> Dict[int, Tuple[str, List[Tuple[str, int, int]]]]:\n    texts_and_annotations = dict()\n    counter = 0\n    with codecs.open(fname, mode='r', encoding='utf-8') as fp:\n        cur_line = fp.readline()\n        while len(cur_line) > 0:\n            prep_line = cur_line.strip()\n            if len(prep_line) > 0:\n                identifier, text, ners = parse_runne_json_string(prep_line)\n                if identifier in texts_and_annotations:\n                    err_msg = f'Identifier {identifier} is duplicated!'\n                    raise ValueError(err_msg)\n                ners = sorted(\n                    list(set(ners)),\n                    key=lambda it: (it[1], it[2], it[0])\n                )\n                texts_and_annotations[identifier] = (text, ners)\n            cur_line = fp.readline()\n    return texts_and_annotations","metadata":{"cellId":"rdk61dszcjdjj42724vsp","id":"d3f3cf22","execution":{"iopub.status.busy":"2022-05-30T00:11:38.046697Z","iopub.execute_input":"2022-05-30T00:11:38.047051Z","iopub.status.idle":"2022-05-30T00:11:38.055755Z","shell.execute_reply.started":"2022-05-30T00:11:38.04702Z","shell.execute_reply":"2022-05-30T00:11:38.054222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef calc_entity_freqs(data: Dict[int, Tuple[str, List[Tuple[str, int, int]]]],\n                      id_list: Union[List[int], None] = None) -> Dict[str, int]:\n    frequencies = dict()\n    re_for_entity = re.compile(r'^[A-Z]+[_A-Z]*[A-Z]+$')\n    if id_list is None:\n        id_list_ = list(data.keys())\n    else:\n        id_list_ = id_list\n    for identifier in id_list_:\n        _, ners = data[identifier]\n        for ne_type, _, _ in ners:\n            err_msg = f'{ne_type} is inadmissible named entity class!'\n            if ne_type.startswith('B-') or ne_type.startswith('I-') or \\\n                    (ne_type == 'O'):\n                raise ValueError(err_msg)\n            if re_for_entity.search(ne_type) is None:\n                raise ValueError(err_msg)\n            frequencies[ne_type] = frequencies.get(ne_type, 0) + 1\n    return frequencies","metadata":{"cellId":"8yky3nr44o5gdsfrxzbx9","id":"dd5375c5","execution":{"iopub.status.busy":"2022-05-30T00:00:33.533002Z","iopub.execute_input":"2022-05-30T00:00:33.533379Z","iopub.status.idle":"2022-05-30T00:00:33.542118Z","shell.execute_reply.started":"2022-05-30T00:00:33.533333Z","shell.execute_reply":"2022-05-30T00:00:33.541058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef tokenize_text_with_ners(s: str, tokenizer: BertTokenizer,\n                            ners: List[Tuple[str, int, int]],\n                            ne_vocabulary: List[str]) \\\n        -> Tuple[List[str], List[List[int]]]:\n    words, subtokens, subtoken_bounds = tokenize_text(s, tokenizer)\n    word_bounds = []\n    for cur_word, word_start, word_end in words:\n        word_bounds.append((\n            subtoken_bounds[word_start][0],\n            subtoken_bounds[word_end - 1][1]\n        ))\n    ne_indicators = []\n    for _ in range(len(ne_vocabulary)):\n        ne_indicators.append([0 for _ in range(len(subtokens))])\n    ne_set = set(map(lambda it: it[0], ners))\n    if len(ne_set) == 0:\n        return subtokens, ne_indicators\n    diff_ne = ne_set - set(ne_vocabulary)\n    if len(diff_ne) > 0:\n        err_msg = f'The annotation {ners} is wrong because ' \\\n                  f'it contains unknown entities! ' \\\n                  f'They are: {sorted(list(diff_ne))}.'\n        raise ValueError(err_msg)\n    for ne_class, ne_start, ne_end in ners:\n        ne_id = ne_vocabulary.index(ne_class)\n        start_word_idx = -1\n        for word_idx, (word_start, word_end) in enumerate(word_bounds):\n            if (ne_start >= word_start) and (ne_start < word_end):\n                if ne_start != word_start:\n                    warn_msg = f'The annotation {ners} can have errors. ' \\\n                               f'The entity {(ne_class, ne_start, ne_end)} ' \\\n                               f'is inexactly found in the text \"{s}\". ' \\\n                               f'{ne_start} != {word_start}'\n                    warnings.warn(warn_msg)\n                start_word_idx = word_idx\n                break\n        if start_word_idx < 0:\n            err_msg = f'The annotation {ners} is wrong. ' \\\n                      f'The entity {(ne_class, ne_start, ne_end)} ' \\\n                      f'is not found in the text {s}.'\n            raise ValueError(err_msg)\n        end_word_idx = -1\n        for word_idx, (word_start, word_end) in enumerate(word_bounds):\n            if (ne_end > word_start) and (ne_end <= word_end):\n                if ne_end != word_end:\n                    warn_msg = f'The annotation {ners} can have errors. ' \\\n                               f'The entity {(ne_class, ne_start, ne_end)} ' \\\n                               f'is inexactly found in the text \"{s}\". ' \\\n                               f'{ne_end} != {word_end}'\n                    warnings.warn(warn_msg)\n                end_word_idx = word_idx\n                break\n        if end_word_idx < 0:\n            err_msg = f'The annotation {ners} is wrong. ' \\\n                      f'The entity {(ne_class, ne_start, ne_end)} ' \\\n                      f'is not found in the text {s}.'\n            raise ValueError(err_msg)\n        init_ne_subtoken = words[start_word_idx][1]\n        fin_ne_subtoken = words[end_word_idx][2]\n        for subtoken_idx in range(init_ne_subtoken, fin_ne_subtoken):\n            ne_indicators[ne_id][subtoken_idx] = 1\n        ne_indicators[ne_id][init_ne_subtoken] = 2\n    return subtokens, ne_indicators","metadata":{"cellId":"8augfnppnd9je6fv0g6ow8","id":"e85c9c44","execution":{"iopub.status.busy":"2022-05-30T00:00:33.993511Z","iopub.execute_input":"2022-05-30T00:00:33.99385Z","iopub.status.idle":"2022-05-30T00:00:34.012403Z","shell.execute_reply.started":"2022-05-30T00:00:33.993822Z","shell.execute_reply":"2022-05-30T00:00:34.011538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef tokenize_text(s: str, tokenizer: BertTokenizer) \\\n        -> Tuple[List[Tuple[str, int, int]], List[str],\n                 List[Union[None, Tuple[int, int]]]]:\n    words: List[Tuple[str, int, int]] = []\n    subtokens: List[str] = []\n    subtoken_bounds: List[Union[None, Tuple[int, int]]] = []\n    subtokens.append(tokenizer.cls_token)\n    subtoken_bounds.append(None)\n    n_bpe = 1\n    tokenization_iterator = filter(\n        lambda it2: len(s[it2[0]:it2[1]].strip()) > 0,\n        map(\n            lambda it1: (tuple(it1)[0], tuple(it1)[1]),\n            tokenize(s.replace('​', ' '))\n        )\n    )\n    word_bounds = []\n    punctuation = {',', '-', ':', ';', '.', ')', '(', '\\]', '[', '<', '>',\n                   '=', '+', '?', '!'}\n    for start_word_pos, end_word_pos in tokenization_iterator:\n        cur_word = s[start_word_pos:end_word_pos]\n        if len(cur_word.strip()) > 0:\n            wordpart_start = -1\n            for char_idx, char_val in enumerate(cur_word):\n                if char_val in punctuation:\n                    if wordpart_start >= 0:\n                        word_bounds.append((\n                            start_word_pos + wordpart_start,\n                            start_word_pos + char_idx\n                        ))\n                        wordpart_start = -1\n                    word_bounds.append((\n                        start_word_pos + char_idx,\n                        start_word_pos + char_idx + 1\n                    ))\n                else:\n                    if wordpart_start < 0:\n                        wordpart_start = char_idx\n            if wordpart_start >= 0:\n                word_bounds.append((\n                    start_word_pos + wordpart_start,\n                    start_word_pos + len(cur_word)\n                ))\n    for start_word_pos, end_word_pos in word_bounds:\n        cur_word = s[start_word_pos:end_word_pos]\n        bpe = tokenizer.tokenize(cur_word)\n        if len(bpe) == 0:\n            err_msg = f'The word \"{cur_word}\" cannot be tokenized!'\n            raise ValueError(err_msg)\n        if tokenizer.unk_token in bpe:\n            subtokens.append(tokenizer.unk_token)\n            subtoken_bounds.append((start_word_pos, end_word_pos))\n            words.append((cur_word, n_bpe, n_bpe + 1))\n            n_bpe += 1\n        elif len(bpe) > 1:\n            prep_word = remove_accents(cur_word.lower())\n            subword_start_pos = 0\n            for src in bpe:\n                if src.startswith('##'):\n                    prep = src[2:]\n                else:\n                    prep = src\n                prep = remove_accents(prep.lower()).replace('`', '')\n                found_start, found_end = find_substring(\n                    s=prep_word[subword_start_pos:],\n                    substring=prep\n                )\n                if (found_start < 0) or (found_end < 0):\n                    err_msg = f'The text {s} cannot be tokenized! \"{prep}\" is ' \\\n                              f'not found in the \"{prep_word}\" from ' \\\n                              f'{subword_start_pos}. Subwords are: {bpe}'\n                    raise ValueError(err_msg)\n                subword_start_pos += found_start\n                subword_end_pos = subword_start_pos + (found_end - found_start)\n                subtokens.append(src)\n                subtoken_bounds.append((\n                    start_word_pos + subword_start_pos,\n                    start_word_pos + subword_end_pos\n                ))\n                subword_start_pos = subword_end_pos\n            if (subtoken_bounds[-1][1] - start_word_pos) < len(prep_word):\n                subtoken_bounds[-1] = (\n                    subtoken_bounds[-1][0],\n                    len(prep_word) + start_word_pos\n                )\n            words.append((cur_word, n_bpe, n_bpe + len(bpe)))\n            n_bpe += len(bpe)\n        else:\n            subtokens.append(bpe[0])\n            subtoken_bounds.append((start_word_pos, end_word_pos))\n            words.append((cur_word, n_bpe, n_bpe + 1))\n            n_bpe += 1\n    subtokens.append(tokenizer.sep_token)\n    subtoken_bounds.append(None)\n    return words, subtokens, subtoken_bounds","metadata":{"cellId":"h0wjfzqgaxiwjovg74hu2m","id":"5cb8702e","execution":{"iopub.status.busy":"2022-05-30T00:00:34.449701Z","iopub.execute_input":"2022-05-30T00:00:34.45005Z","iopub.status.idle":"2022-05-30T00:00:34.471972Z","shell.execute_reply.started":"2022-05-30T00:00:34.45002Z","shell.execute_reply":"2022-05-30T00:00:34.470863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef train_test_split(data: Dict[int, Tuple[str, List[Tuple[str, int, int]]]]) \\\n        -> Tuple[Dict[int, Tuple[str, List[Tuple[str, int, int]]]],\n                 Dict[int, Tuple[str, List[Tuple[str, int, int]]]]]:\n    frequencies = calc_entity_freqs(data)\n    identifiers = sorted(list(data.keys()))\n    print(f'There are {len(frequencies)} named entity classes:')\n    max_txt_width = max(map(lambda it: len(it), frequencies))\n    max_num_width = max(map(lambda it: len(str(frequencies[it])), frequencies))\n    sorted_entity_list = sorted(\n        list(frequencies.keys()),\n        key=lambda it: (-frequencies[it], it)\n    )\n    for named_entity in sorted_entity_list:\n        freq = frequencies[named_entity]\n        if freq < 15:\n            err_msg = f'The data cannot be splitted because ' \\\n                      f'the entity {named_entity} is too rare ' \\\n                      f'(its frequency is {freq}).'\n            raise ValueError(err_msg)\n        print('  {0:<{1}} {2:>{3}}'.format(named_entity, max_txt_width,\n                                           freq, max_num_width))\n    print('')\n    random.shuffle(identifiers)\n    n = int(round(0.15 * float(len(identifiers))))\n    training_frequencies = calc_entity_freqs(data, identifiers[n:])\n    test_frequencies = calc_entity_freqs(data, identifiers[:n])\n    if set(training_frequencies.keys()) == set(test_frequencies.keys()):\n        ok = True\n        for it in training_frequencies:\n            ratio = training_frequencies[it] / float(frequencies[it])\n            if ratio < 0.1:\n                ok = False\n                break\n        if ok:\n            for it in test_frequencies:\n                ratio = test_frequencies[it] / float(frequencies[it])\n                if ratio < 0.1:\n                    ok = False\n                    break\n    else:\n        ok = False\n    if not ok:\n        for _ in range(1000):\n            random.shuffle(identifiers)\n            training_frequencies = calc_entity_freqs(data, identifiers[n:])\n            test_frequencies = calc_entity_freqs(data, identifiers[:n])\n            if set(training_frequencies.keys()) == set(test_frequencies.keys()):\n                ok = True\n                for it in training_frequencies:\n                    ratio = training_frequencies[it] / float(frequencies[it])\n                    if ratio < 0.1:\n                        ok = False\n                        break\n                if ok:\n                    for it in test_frequencies:\n                        ratio = test_frequencies[it] / float(frequencies[it])\n                        if ratio < 0.1:\n                            ok = False\n                            break\n            else:\n                ok = False\n            if ok:\n                break\n    if not ok:\n        err_msg = 'The data cannot be splitted.'\n        raise ValueError(err_msg)\n    data_for_training = dict()\n    data_for_testing = dict()\n    for it in identifiers[n:]:\n        data_for_training[it] = data[it]\n    for it in identifiers[:n]:\n        data_for_testing[it] = data[it]\n    print('For training:')\n    for named_entity in sorted_entity_list:\n        freq = training_frequencies[named_entity]\n        print('  {0:<{1}} {2:>{3}}'.format(named_entity, max_txt_width,\n                                           freq, max_num_width))\n    print('')\n    print('For testing:')\n    for named_entity in sorted_entity_list:\n        freq = test_frequencies[named_entity]\n        print('  {0:<{1}} {2:>{3}}'.format(named_entity, max_txt_width,\n                                           freq, max_num_width))\n    print('')\n    return data_for_training, data_for_testing","metadata":{"cellId":"6chila9yaey1zmbmjgix87","id":"21eae9da","execution":{"iopub.status.busy":"2022-05-30T00:00:34.918487Z","iopub.execute_input":"2022-05-30T00:00:34.921782Z","iopub.status.idle":"2022-05-30T00:00:34.963218Z","shell.execute_reply.started":"2022-05-30T00:00:34.921729Z","shell.execute_reply":"2022-05-30T00:00:34.962271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\nSENTENIZE_EXCLUSIONS = [\n    'st.',\n    'св.',\n    'г.',\n    'с.',\n    'род.',\n    'рожд.'\n]","metadata":{"cellId":"9ah768lhddeyks4ymdt3wk","id":"b85c8d18","execution":{"iopub.status.busy":"2022-05-30T00:00:35.39251Z","iopub.execute_input":"2022-05-30T00:00:35.39288Z","iopub.status.idle":"2022-05-30T00:00:35.397357Z","shell.execute_reply.started":"2022-05-30T00:00:35.392849Z","shell.execute_reply":"2022-05-30T00:00:35.396461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef find_quoted_substrings(s: str) -> List[Tuple[int, int]]:\n    span_start = -1\n    spans = []\n    for char_idx, char_val in enumerate(s):\n        if char_val in {'\"', '\\''}:\n            if span_start < 0:\n                span_start = char_idx\n            else:\n                span_end = char_idx + 1\n                spans.append((span_start, span_end))\n                span_start = -1\n        elif char_val == '«':\n            if span_start < 0:\n                span_start = char_idx\n        elif char_val == '»':\n            if span_start >= 0:\n                span_end = char_idx + 1\n                spans.append((span_start, span_end))\n                span_start = -1\n    return spans","metadata":{"cellId":"1d2y2v07b4kh2eauhulhdcz","id":"9f67821f","execution":{"iopub.status.busy":"2022-05-30T00:00:35.888506Z","iopub.execute_input":"2022-05-30T00:00:35.888962Z","iopub.status.idle":"2022-05-30T00:00:35.896427Z","shell.execute_reply.started":"2022-05-30T00:00:35.888928Z","shell.execute_reply":"2022-05-30T00:00:35.895619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef find_span(spans: List[Tuple[int, int]], char_position: int) -> int:\n    found_idx = -1\n    for span_idx, (span_start, span_end) in enumerate(spans):\n        if (char_position >= span_start) and (char_position < span_end):\n            found_idx = span_idx\n            break\n    return found_idx","metadata":{"cellId":"gf29mzvms8gtxh4hoi1lc","id":"0a75f2b7","execution":{"iopub.status.busy":"2022-05-30T00:00:36.453039Z","iopub.execute_input":"2022-05-30T00:00:36.453741Z","iopub.status.idle":"2022-05-30T00:00:36.460324Z","shell.execute_reply.started":"2022-05-30T00:00:36.453704Z","shell.execute_reply":"2022-05-30T00:00:36.459247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef is_exclusion(s: str) -> bool:\n    prep = s.lower()\n    ok = False\n    for cur_exclusion in SENTENIZE_EXCLUSIONS:\n        found_idx = prep.rfind(cur_exclusion)\n        if found_idx >= 0:\n            if prep[found_idx:] == cur_exclusion:\n                if found_idx > 0:\n                    ok = (not prep[found_idx - 1].isalnum())\n                else:\n                    ok = True\n        if ok:\n            break\n    return ok\n","metadata":{"cellId":"nc7gl2qbddcohnucfpiva","id":"9a853ee5","execution":{"iopub.status.busy":"2022-05-30T00:00:37.052728Z","iopub.execute_input":"2022-05-30T00:00:37.053583Z","iopub.status.idle":"2022-05-30T00:00:37.061209Z","shell.execute_reply.started":"2022-05-30T00:00:37.053546Z","shell.execute_reply":"2022-05-30T00:00:37.060365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef sentenize_with_exclusions(s: str) -> List[Tuple[int, int]]:\n    sentence_bounds = list(map(\n        lambda it: (tuple(it)[0], tuple(it)[1]),\n        sentenize(s)\n    ))\n    if len(sentence_bounds) == 0:\n        return sentence_bounds\n    prepared_sentence_bounds = [sentence_bounds[0]]\n    prev_text = s[sentence_bounds[0][0]:sentence_bounds[0][1]].lower()\n    for cur_bounds in sentence_bounds[1:]:\n        if is_exclusion(prev_text):\n\n            prepared_sentence_bounds[-1] = (\n                prepared_sentence_bounds[-1][0],\n                cur_bounds[1]\n            )\n      \n        else:\n            prepared_sentence_bounds.append(cur_bounds)\n        prev_text = s[cur_bounds[0]:cur_bounds[1]].lower()\n    quote_spans = find_quoted_substrings(s)\n    if len(quote_spans) == 0:\n        return prepared_sentence_bounds\n    for quote_span_start, quote_span_end in quote_spans:\n        first_sent_idx = find_span(prepared_sentence_bounds, quote_span_start)\n        last_sent_idx = find_span(prepared_sentence_bounds, quote_span_end - 1)\n        if (first_sent_idx < 0) or (last_sent_idx < 0):\n            raise ValueError(f'The sentence \"{s}\" has incorrect spans!')\n        if first_sent_idx < last_sent_idx:\n            prepared_sentence_bounds[first_sent_idx] = (\n                prepared_sentence_bounds[first_sent_idx][0],\n                prepared_sentence_bounds[last_sent_idx][1]\n            )\n            prepared_sentence_bounds = \\\n                prepared_sentence_bounds[:(first_sent_idx + 1)] + \\\n                prepared_sentence_bounds[(last_sent_idx + 1):]\n    return prepared_sentence_bounds","metadata":{"cellId":"bo19qrzsnhab19rf6wxv3u","id":"3275f67f","execution":{"iopub.status.busy":"2022-05-30T00:00:39.468888Z","iopub.execute_input":"2022-05-30T00:00:39.469695Z","iopub.status.idle":"2022-05-30T00:00:39.480123Z","shell.execute_reply.started":"2022-05-30T00:00:39.46966Z","shell.execute_reply":"2022-05-30T00:00:39.479329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef sentenize_text(s: str) -> List[Tuple[int, int]]:\n    sent_start = -1\n    sentence_bounds = []\n    newline_counter = 0\n    last_char = ''\n    for char_idx, char_val in enumerate(s):\n        if char_val in {'\\n', '\\r'}:\n            newline_counter += 1\n        else:\n            if not char_val.isspace():\n                if sent_start < 0:\n                    sent_start = char_idx\n                else:\n                    if newline_counter > 0:\n                        if last_char in {'?', '!'}:\n                            sent_end = char_idx\n                        elif char_val.istitle() or (last_char == '.'):\n                            sent_end = char_idx\n                        else:\n                            sent_end = -1\n                        if sent_end >= 0:\n                            while sent_end > sent_start:\n                                if not s[sent_end - 1].isspace():\n                                    break\n                                sent_end -= 1\n                            if sent_end > sent_start:\n                                text = s[sent_start:sent_end].replace('​', ' ')\n                                if len(text.strip()) > 0:\n                                    for it in sentenize_with_exclusions(text):\n                                        sentence_bounds.append((\n                                            sent_start + it[0],\n                                            sent_start + it[1]\n                                        ))\n                            sent_start = char_idx\n                        newline_counter = 0\n                last_char = char_val\n    if sent_start >= 0:\n        sent_end = len(s)\n        while sent_end > sent_start:\n            if not s[sent_end - 1].isspace():\n                break\n            sent_end -= 1\n        if sent_end > sent_start:\n            text = s[sent_start:sent_end].replace('​', ' ')\n            if len(text.strip()) > 0:\n                for it in sentenize_with_exclusions(text):\n                    sentence_bounds.append((\n                        sent_start + it[0],\n                        sent_start + it[1]\n                    ))\n    return sentence_bounds\n    ","metadata":{"cellId":"jmrmu4twrmasm8m9j0e35j","id":"550e16b5","execution":{"iopub.status.busy":"2022-05-30T00:00:40.487811Z","iopub.execute_input":"2022-05-30T00:00:40.488681Z","iopub.status.idle":"2022-05-30T00:00:40.503002Z","shell.execute_reply.started":"2022-05-30T00:00:40.488645Z","shell.execute_reply":"2022-05-30T00:00:40.50216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef find_substring(s: str, substring: str) -> Tuple[int, int]:\n    if '`' in substring:\n        err_msg = f'\"{substring}\" is a wrong sub-word, because ' \\\n                  f'it contains \"`\". It cannot be found in the string \"{s}\".'\n        raise ValueError(err_msg)\n    if substring != substring.strip():\n        err_msg = f'\"{substring}\" is a wrong sub-word, because ' \\\n                  f'it includes initial and/or final spaces.'\n        raise ValueError(err_msg)\n    if len(substring) == 0:\n        return -1, -1\n    if '`' not in s:\n        start_pos = s.find(substring)\n        if start_pos < 0:\n            return -1, -1\n        return start_pos, start_pos + len(substring)\n    found_idx = s.find(substring[0])\n    if found_idx < 0:\n        return -1, -1\n    idx1 = found_idx + 1\n    if found_idx > 0:\n        while found_idx > 0:\n            if s[found_idx - 1] != '`':\n                break\n            found_idx -= 1\n    for idx2 in range(1, len(substring)):\n        while idx1 < len(s):\n            if s[idx1] != '`':\n                break\n            idx1 += 1\n        if idx1 >= len(s):\n            break\n        if s[idx1] != substring[idx2]:\n            break\n        idx1 += 1\n    if s[found_idx:idx1].replace('`', '') != substring:\n        return -1, -1\n    while idx1 < len(s):\n        if s[idx1] != '`':\n            break\n        idx1 += 1\n    return found_idx, idx1","metadata":{"cellId":"47jr9754pgjgyv0odl4z4t","id":"ff0e4ce1","execution":{"iopub.status.busy":"2022-05-30T00:00:42.084849Z","iopub.execute_input":"2022-05-30T00:00:42.085203Z","iopub.status.idle":"2022-05-30T00:00:42.096154Z","shell.execute_reply.started":"2022-05-30T00:00:42.085172Z","shell.execute_reply":"2022-05-30T00:00:42.095107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef remove_accents(s: str) -> str:\n    res = []\n    for c in s:\n        norm = ud.normalize('NFKD', c)\n        if len(norm) > 1:\n            new_res = list(filter(lambda it: ud.combining(it) == 0, norm))\n            if len(new_res) == 0:\n                res.append('`')\n            else:\n                res.append(new_res[0])\n        elif len(norm) == 1:\n            if ud.combining(norm) == 0:\n                res.append(norm)\n            else:\n                res.append('`')\n        else:\n            res.append('`')\n    return \"\".join(res)","metadata":{"cellId":"ra651i3lky18u2pyo8po7","id":"91b00078","execution":{"iopub.status.busy":"2022-05-30T00:00:42.59266Z","iopub.execute_input":"2022-05-30T00:00:42.593114Z","iopub.status.idle":"2022-05-30T00:00:42.59998Z","shell.execute_reply.started":"2022-05-30T00:00:42.593083Z","shell.execute_reply":"2022-05-30T00:00:42.599236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef sentenize_text_with_ners(s: str, tokenizer: BertTokenizer,\n                             ners: List[Tuple[str, int, int]],\n                             ne_vocabulary: List[str]) \\\n        -> List[Tuple[List[str], List[List[int]]]]:\n    if len(ners) != len(set(ners)):\n        raise ValueError('Some entities are duplicated!')\n    sentence_bounds = sentenize_text(s)\n    res = []\n    used_entities = set()\n    for sent_start, sent_end in sentence_bounds:\n        ners_for_sent = []\n        for ne_type, ne_start, ne_end in ners:\n            if ne_end <= sent_start:\n                continue\n            if ne_start >= sent_end:\n                continue\n            if (ne_start < sent_start) or (ne_end > sent_end):\n                err_msg = f'The entity ({ne_type}, {ne_start}, {ne_end}) ' \\\n                          f'is wrong! It is located in more than ' \\\n                          f'a single sentence. More probably sentence is ' \\\n                          f'\"{s[sent_start:sent_end]}\"'\n                raise ValueError(err_msg)\n            if (ne_type, ne_start, ne_end) in used_entities:\n                err_msg = f'The entity ({ne_type}, {ne_start}, {ne_end}) ' \\\n                          f'is wrong! It is located in more than ' \\\n                          f'a single sentence. More probably sentence is ' \\\n                          f'\"{s[sent_start:sent_end]}\"'\n                raise ValueError(err_msg)\n            ners_for_sent.append(\n                (\n                    ne_type,\n                    ne_start - sent_start,\n                    ne_end - sent_start\n                )\n            )\n            used_entities.add((ne_type, ne_start, ne_end))\n        res.append(tokenize_text_with_ners(s[sent_start:sent_end], tokenizer,\n                                           ners_for_sent, ne_vocabulary))\n    if len(used_entities) != len(ners):\n        err_msg = f'Some entities are not used! They are: ' \\\n                  f'{sorted(list(set(ners) - used_entities))}'\n        raise ValueError(err_msg)\n    return res","metadata":{"cellId":"ff20p5q95gn0auqp46rjgvh","id":"e2db147c","execution":{"iopub.status.busy":"2022-05-30T00:00:43.0924Z","iopub.execute_input":"2022-05-30T00:00:43.093353Z","iopub.status.idle":"2022-05-30T00:00:43.103901Z","shell.execute_reply.started":"2022-05-30T00:00:43.093281Z","shell.execute_reply":"2022-05-30T00:00:43.103177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_trainset_for_ner(data: Dict[int,\n                                      Tuple[str, List[Tuple[str, int, int]]]],\n                           tokenizer: BertTokenizer, max_seq_len: int,\n                           entities: List[str]) \\\n        -> Tuple[np.ndarray, List[np.ndarray]]:\n    if 'O' in entities:\n        err_msg = f'The entities list {entities} is wrong ' \\\n                  f'because it contains the `O` entity.'\n        raise ValueError(err_msg)\n    list_of_tokenized_texts = []\n    list_of_ne_indicators = []\n    max_seq_len_ = max_seq_len\n    print(f'Number of texts is {len(data)}.')\n    for cur_id in tqdm(sorted(list(data.keys()))):\n        text, ners = data[cur_id]\n        batch = sentenize_text_with_ners(\n            s=text,\n            tokenizer=tokenizer,\n            ners=ners,\n            ne_vocabulary=entities\n        )\n        for tokenized_text, ne_indicators in batch:\n            list_of_tokenized_texts.append(tokenized_text)\n            list_of_ne_indicators.append(ne_indicators)\n            if len(tokenized_text) > max_seq_len_:\n                max_seq_len_ = len(tokenized_text)\n    print(f'Number of sentences is {len(list_of_tokenized_texts)}.')\n    X = []\n    y = [[] for _ in range(len(entities))]\n    for tokenized_text, ne_indicators in zip(list_of_tokenized_texts,\n                                             list_of_ne_indicators):\n        ne_indicators_ = copy.copy(ne_indicators)\n        while len(tokenized_text) < max_seq_len_:\n            tokenized_text.append(tokenizer.pad_token)\n            for ne_id in range(len(entities)):\n                ne_indicators_[ne_id].append(0)\n        X.append(tokenizer.convert_tokens_to_ids(tokenized_text))\n        for ne_id in range(len(entities)):\n            y[ne_id].append(\n                transform_indicator_to_classmatrix(ne_indicators_[ne_id])\n            )\n        del ne_indicators_\n    X = np.array(X, dtype=np.int32)\n    y = [np.concatenate(cur, axis=0) for cur in y]\n    if X.shape[1] == max_seq_len:\n        return X, y\n    indices_of_long_texts = []\n    for sample_idx in range(X.shape[0]):\n        is_padding = True\n        for token_idx in range(max_seq_len, X.shape[1]):\n            if X[sample_idx, token_idx] != tokenizer.pad_token_id:\n                is_padding = False\n                break\n        if not is_padding:\n            indices_of_long_texts.append(sample_idx)\n    iteration = 1\n    while len(indices_of_long_texts) > 0:\n        print(f'Iter {iteration}: '\n              f'there are {len(indices_of_long_texts)} very long texts!')\n        new_X = np.full(\n            shape=(len(indices_of_long_texts), max_seq_len_),\n            fill_value=tokenizer.pad_token_id,\n            dtype=np.int32\n        )\n        new_y = [np.zeros((len(indices_of_long_texts), max_seq_len_, 5),\n                          dtype=np.float32) for _ in range(len(y))]\n        ndiff = max_seq_len_ - max_seq_len\n        for local_idx, global_idx in enumerate(indices_of_long_texts):\n            new_X[local_idx, 0:ndiff] = X[global_idx, max_seq_len:]\n            X[global_idx, max_seq_len:] = tokenizer.pad_token_id\n            for output_idx in range(len(y)):\n                new_y[output_idx][local_idx, 0:ndiff, :] = \\\n                    y[output_idx][global_idx, max_seq_len:, :]\n                y[output_idx][global_idx, max_seq_len:, :] = 0.0\n        X = np.concatenate((X, new_X), axis=0)\n        y = [np.concatenate((y[output_idx], new_y[output_idx]), axis=0)\n             for output_idx in range(len(y))]\n        indices_of_long_texts = []\n        for sample_idx in range(X.shape[0]):\n            is_padding = True\n            for token_idx in range(max_seq_len, X.shape[1]):\n                if X[sample_idx, token_idx] != tokenizer.pad_token_id:\n                    is_padding = False\n                    break\n            if not is_padding:\n                indices_of_long_texts.append(sample_idx)\n        iteration += 1\n    X = X[:, :max_seq_len]\n    y = [cur[:, :max_seq_len, :] for cur in y]\n    print(f'Number of sentences after cutting is {X.shape[0]}.')\n    return X, y","metadata":{"cellId":"fd33m1uj9tbpxjuw9ixj3a","id":"f7da613a","execution":{"iopub.status.busy":"2022-05-30T00:00:43.681538Z","iopub.execute_input":"2022-05-30T00:00:43.682202Z","iopub.status.idle":"2022-05-30T00:00:43.703894Z","shell.execute_reply.started":"2022-05-30T00:00:43.682171Z","shell.execute_reply":"2022-05-30T00:00:43.703031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef calc_features(tokenizer: BertTokenizer, feature_extractor: TFBertModel,\n                  max_sent_len: int, source_text: str) -> \\\n        Tuple[List[Tuple[str, int, int]], np.ndarray]:\n    word_features = []\n    all_words = []\n    for sent_start, sent_end in sentenize_text(source_text):\n        words, subtokens, subtoken_bounds = tokenize_text(\n            s=source_text[sent_start:sent_end],\n            tokenizer=tokenizer\n        )\n        while (len(subtokens) % max_sent_len) != 0:\n            subtokens.append(tokenizer.pad_token)\n            subtoken_bounds.append(None)\n        x = []\n        start_pos = 0\n        for _ in range(len(subtokens) // max_sent_len):\n            end_pos = start_pos + max_sent_len\n            subtoken_indices = tokenizer.convert_tokens_to_ids(\n                subtokens[start_pos:end_pos]\n            )\n            x.append(\n                np.array(\n                    subtoken_indices,\n                    dtype=np.int32\n                ).reshape((1, max_sent_len))\n            )\n            start_pos = end_pos\n        predicted = feature_extractor.predict(np.vstack(x), batch_size=1)[0]\n        if not isinstance(predicted, np.ndarray):\n            predicted = predicted.numpy()\n        if len(predicted.shape) != 3:\n            err_msg = f'The predicted feature matrix is wrong! ' \\\n                      f'Expected 3-D array, got {len(predicted.shape)}-D one.'\n            raise ValueError(err_msg)\n        if predicted.shape[0] != (len(subtokens) // max_sent_len):\n            err_msg = f'The predicted feature matrix does not correspond to' \\\n                      f' the input data! {predicted.shape[0]} != ' \\\n                      f'{len(subtokens) // max_sent_len}.'\n            raise ValueError(err_msg)\n        subtoken_features = [predicted[0]]\n        for idx in range(1, predicted.shape[0]):\n            subtoken_features.append(predicted[idx])\n        subtoken_features = np.vstack(subtoken_features)\n        del predicted\n        for cur_word, word_start, word_end in words:\n            word_features.append(\n                np.mean(subtoken_features[word_start:word_end],\n                        axis=0, keepdims=True)\n            )\n            all_words.append((\n                cur_word,\n                subtoken_bounds[word_start][0] + sent_start,\n                subtoken_bounds[word_end - 1][1] + sent_start\n            ))\n    return all_words, np.vstack(word_features)","metadata":{"cellId":"64py5vylvl3w5lo9u0e67e","id":"d607eefe","execution":{"iopub.status.busy":"2022-05-30T00:00:44.221418Z","iopub.execute_input":"2022-05-30T00:00:44.221761Z","iopub.status.idle":"2022-05-30T00:00:44.23543Z","shell.execute_reply.started":"2022-05-30T00:00:44.221731Z","shell.execute_reply":"2022-05-30T00:00:44.234564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef find_entity_words(words: List[Tuple[str, int, int]],\n                      entity_start: int, entity_end: int) -> Tuple[int, int]:\n    start_word_idx = -1\n    end_word_idx = -1\n    for word_idx, (_, word_start, word_end) in enumerate(words):\n        if entity_start < word_end:\n            if start_word_idx < 0:\n                start_word_idx = word_idx\n        if entity_end > word_start:\n            end_word_idx = word_idx\n        if word_start >= entity_end:\n            break\n    if (start_word_idx < 0) or (end_word_idx < 0):\n        return -1, -1\n    true_entity_start = words[start_word_idx][1]\n    true_entity_end = words[end_word_idx][2]\n    if entity_end <= true_entity_start:\n        return -1, -1\n    if entity_start >= true_entity_end:\n        return -1, -1\n    return start_word_idx, end_word_idx + 1","metadata":{"cellId":"wi9gzgnf4t7lsjfe6eqvz","id":"c1c73837","execution":{"iopub.status.busy":"2022-05-30T00:00:44.776534Z","iopub.execute_input":"2022-05-30T00:00:44.777148Z","iopub.status.idle":"2022-05-30T00:00:44.784191Z","shell.execute_reply.started":"2022-05-30T00:00:44.777113Z","shell.execute_reply":"2022-05-30T00:00:44.783174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\ndef calc_features_and_labels(tokenizer: BertTokenizer,\n                             feature_extractor: TFBertModel, max_sent_len: int,\n                             ne_list: List[str], source_text: str,\n                             annotation: List[Tuple[str, int, int]]) -> \\\n        Tuple[np.ndarray, List[np.ndarray]]:\n    words, features = calc_features(tokenizer, feature_extractor, max_sent_len,\n                                    source_text)\n    named_entities = [np.zeros((len(words), 5), dtype=np.float32)\n                      for _ in range(len(ne_list))]\n    for word_idx in range(len(words)):\n        for named_entity_id in range(len(ne_list)):\n            named_entities[named_entity_id][word_idx, 0] = 1.0\n    for entity_class, entity_char_start, entity_char_end in annotation:\n        try:\n            named_entity_id = ne_list.index(entity_class)\n        except:\n            named_entity_id = -1\n        if named_entity_id < 0:\n            err_msg = f'The entity class \"{entity_class}\" is unknown!'\n            raise ValueError(err_msg)\n        entity_start, entity_end = find_entity_words(words, entity_char_start,\n                                                     entity_char_end)\n        if (entity_start < 0) or (entity_end < 0):\n            unknown_entity = (entity_class, entity_char_start, entity_char_end)\n            input_text = source_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n            err_msg = f'The entity {unknown_entity} is not found in the text ' \\\n                      f'\"{input_text}\", tokenized by the following words: ' \\\n                      f'{words}.'\n            raise ValueError(err_msg)\n        if entity_end - entity_start > 1:\n            named_entities[named_entity_id][entity_start, 0] = 0.0\n            named_entities[named_entity_id][entity_start, 1] = 1.0\n            for word_idx in range(entity_start + 1, entity_end - 1):\n                named_entities[named_entity_id][word_idx, 0] = 0.0\n                named_entities[named_entity_id][word_idx, 3] = 1.0\n            named_entities[named_entity_id][entity_end - 1, 0] = 0.0\n            named_entities[named_entity_id][entity_end - 1, 2] = 1.0\n        else:\n            named_entities[named_entity_id][entity_start, 0] = 0.0\n            named_entities[named_entity_id][entity_start, 4] = 1.0\n    return features, named_entities","metadata":{"cellId":"23tivg87y7wpvz3l0ob8eq","id":"4ecb8597","execution":{"iopub.status.busy":"2022-05-30T00:00:45.401692Z","iopub.execute_input":"2022-05-30T00:00:45.402059Z","iopub.status.idle":"2022-05-30T00:00:45.417191Z","shell.execute_reply.started":"2022-05-30T00:00:45.402028Z","shell.execute_reply":"2022-05-30T00:00:45.416347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\nmodel_path = 'DeepPavlov/rubert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"cellId":"wwdrfm0inbbabsnicyb0uw","id":"ce91abe6","outputId":"e6bd8232-4522-480c-c1ab-d14444c3e874","execution":{"iopub.status.busy":"2022-05-30T00:00:46.901143Z","iopub.execute_input":"2022-05-30T00:00:46.903573Z","iopub.status.idle":"2022-05-30T00:00:52.640606Z","shell.execute_reply.started":"2022-05-30T00:00:46.903529Z","shell.execute_reply":"2022-05-30T00:00:52.639773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\nmodel = transformers.TFBertModel.from_pretrained(model_path, \n                                                 output_hidden_states = True, \n                                                 from_pt=True,\n                                                 )","metadata":{"cellId":"4dhbt9mrl7ouaahuz80fc","id":"d2021a8f","outputId":"3144e3b3-755b-478d-8b05-101e99846155","execution":{"iopub.status.busy":"2022-05-30T00:00:52.642237Z","iopub.execute_input":"2022-05-30T00:00:52.64262Z","iopub.status.idle":"2022-05-30T00:01:32.832152Z","shell.execute_reply.started":"2022-05-30T00:00:52.642584Z","shell.execute_reply":"2022-05-30T00:01:32.831341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!g1.1\n\nwith codecs.open(ners_fname, mode='r', encoding='utf-8') as fp:\n        possible_named_entities = list(filter(\n            lambda it2: len(it2) > 0,\n            map(\n                lambda it1: it1.strip(),\n                fp.readlines()\n            )\n        ))","metadata":{"cellId":"m648mrld5hkxtrkx3ag9j","id":"dd62b96b","execution":{"iopub.status.busy":"2022-05-30T00:01:32.833855Z","iopub.execute_input":"2022-05-30T00:01:32.834719Z","iopub.status.idle":"2022-05-30T00:01:32.841643Z","shell.execute_reply.started":"2022-05-30T00:01:32.834674Z","shell.execute_reply":"2022-05-30T00:01:32.840493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_runne_text_and_ners = load_runne_data('train.jsonl?raw=true')","metadata":{"cellId":"bhllgy9f7bw42tvz6xl854","id":"ae562861","execution":{"iopub.status.busy":"2022-05-30T00:11:47.284762Z","iopub.execute_input":"2022-05-30T00:11:47.285174Z","iopub.status.idle":"2022-05-30T00:11:47.742211Z","shell.execute_reply.started":"2022-05-30T00:11:47.28514Z","shell.execute_reply":"2022-05-30T00:11:47.74144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"runne_features = []\nrunne_labels = [[] for _ in range(len(possible_named_entities))]\nfor cur_id in tqdm(sorted(list(train_runne_text_and_ners.keys()))):\n    text, ners = train_runne_text_and_ners[cur_id]\n    X, y = calc_features_and_labels(\n                  tokenizer=tokenizer,\n                  feature_extractor=model, \n                  max_sent_len=128,\n                  ne_list=possible_named_entities,\n                  source_text=text,\n                  annotation=ners,\n              )\n    runne_features.append(X)\n    for idx in range(len(possible_named_entities)):\n        runne_labels[idx].append(y[idx])","metadata":{"cellId":"7zm45qngd5468djr8wdg4r","id":"fb83c937","outputId":"4344069a-9484-4c4d-de6c-3ccb72e20300","execution":{"iopub.status.busy":"2022-05-30T00:11:57.255252Z","iopub.execute_input":"2022-05-30T00:11:57.255908Z","iopub.status.idle":"2022-05-30T00:17:02.812112Z","shell.execute_reply.started":"2022-05-30T00:11:57.255874Z","shell.execute_reply":"2022-05-30T00:17:02.811329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"runne_concat_features = np.vstack([cur for cur in runne_features])\nrunne_concat_labels = runne_labels\nprint('')\nprint(f'X.shape = {runne_concat_features.shape}')\nfor idx in range(len(possible_named_entities)):\n            runne_concat_labels[idx] = np.vstack(runne_labels[idx])\nprint('')\nfor ne_id, ne_cls in enumerate(possible_named_entities):\n            print(f'y[{ne_cls}].shape = {runne_concat_labels[ne_id].shape}')","metadata":{"id":"gzJ6PdLHT20b","outputId":"39474394-333b-4bc6-b79d-51158417d275","execution":{"iopub.status.busy":"2022-05-30T00:17:04.643907Z","iopub.execute_input":"2022-05-30T00:17:04.644725Z","iopub.status.idle":"2022-05-30T00:17:04.827099Z","shell.execute_reply.started":"2022-05-30T00:17:04.644688Z","shell.execute_reply":"2022-05-30T00:17:04.826245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"runne_features_and_labels_train\", 'wb') as fp:\n    pickle.dump(\n        obj=(runne_concat_features, runne_concat_labels),\n        file=fp,\n        protocol=pickle.HIGHEST_PROTOCOL\n    )","metadata":{"id":"06JzrDZ2GUoe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_runne_text_and_ners\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T00:18:31.821148Z","iopub.execute_input":"2022-05-30T00:18:31.822084Z","iopub.status.idle":"2022-05-30T00:18:31.864798Z","shell.execute_reply.started":"2022-05-30T00:18:31.822047Z","shell.execute_reply":"2022-05-30T00:18:31.863605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_read = time.time()\ntrain = pd.read_csv(\"lenta-ru-news.csv.bz2\", usecols=['text'], nrows=1000, dtype={'text': str})\nend_read = time.time() - start_read\nprint(\"Reading time: \", end_read)","metadata":{"id":"4dMZQaltOFnV","outputId":"a6d1a86e-8455-4c3e-e0e4-43d94fba1890","execution":{"iopub.status.busy":"2022-05-30T00:18:38.569993Z","iopub.execute_input":"2022-05-30T00:18:38.570366Z","iopub.status.idle":"2022-05-30T00:18:38.745559Z","shell.execute_reply.started":"2022-05-30T00:18:38.570335Z","shell.execute_reply":"2022-05-30T00:18:38.744704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lenta_features = []\nfor cur_id in tqdm(sorted(list(train.text.keys()))):\n    text = train.text[cur_id]\n    text = text.replace('\\xad', '')\n    text = text.replace('\\n', '') \n    X = calc_features(\n      tokenizer=tokenizer,\n      feature_extractor=model, \n      max_sent_len=128,\n      source_text=text,\n  )\n    lenta_features.append(X)","metadata":{"id":"UO_IpXqwOmQ7","outputId":"be4bcd8f-4975-4deb-93b9-369c6cde16df","execution":{"iopub.status.busy":"2022-05-30T00:18:45.868991Z","iopub.execute_input":"2022-05-30T00:18:45.869353Z","iopub.status.idle":"2022-05-30T00:26:00.552602Z","shell.execute_reply.started":"2022-05-30T00:18:45.86932Z","shell.execute_reply":"2022-05-30T00:26:00.550168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_data = build_trainset_for_ner(\n            data=train_runne_text_and_ners,\n            tokenizer=tokenizer,\n            entities=possible_named_entities,\n            max_seq_len=128,\n        )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T00:26:19.69665Z","iopub.execute_input":"2022-05-30T00:26:19.697009Z","iopub.status.idle":"2022-05-30T00:26:19.920764Z","shell.execute_reply.started":"2022-05-30T00:26:19.69698Z","shell.execute_reply":"2022-05-30T00:26:19.919887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features = np.vstack([cur[1] for cur in lenta_features])\nprint('')\nprint(f'X.shape = {concat_features.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T00:26:32.852714Z","iopub.execute_input":"2022-05-30T00:26:32.853397Z","iopub.status.idle":"2022-05-30T00:26:33.052111Z","shell.execute_reply.started":"2022-05-30T00:26:32.853359Z","shell.execute_reply":"2022-05-30T00:26:33.051255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set = tf.data.Dataset.from_tensor_slices(\n        concat_features,\n    ).batch(16)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T01:54:31.094828Z","iopub.execute_input":"2022-05-30T01:54:31.095413Z","iopub.status.idle":"2022-05-30T01:54:32.251098Z","shell.execute_reply.started":"2022-05-30T01:54:31.09538Z","shell.execute_reply":"2022-05-30T01:54:32.250309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_lenta_token_bounds = np.vstack([cur[0] for cur in lenta_features])\nprint('')\nprint(f'X.shape = {concat_lenta_token_bounds.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T00:28:01.602839Z","iopub.execute_input":"2022-05-30T00:28:01.603203Z","iopub.status.idle":"2022-05-30T00:28:02.033236Z","shell.execute_reply.started":"2022-05-30T00:28:01.603175Z","shell.execute_reply":"2022-05-30T00:28:02.032345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"lenta_concat_features_and_token_bounds_clear\", 'wb') as fp:\n    pickle.dump(\n        obj=(concat_lenta_token_bounds, concat_features_2),\n        file=fp,\n        protocol=pickle.HIGHEST_PROTOCOL\n    )","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:02:06.400803Z","iopub.execute_input":"2022-05-29T12:02:06.401187Z","iopub.status.idle":"2022-05-29T12:02:23.365956Z","shell.execute_reply.started":"2022-05-29T12:02:06.401133Z","shell.execute_reply":"2022-05-29T12:02:23.363769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_addons","metadata":{"execution":{"iopub.status.busy":"2022-05-30T00:49:59.359479Z","iopub.execute_input":"2022-05-30T00:49:59.360274Z","iopub.status.idle":"2022-05-30T00:50:09.162434Z","shell.execute_reply.started":"2022-05-30T00:49:59.360236Z","shell.execute_reply":"2022-05-30T00:50:09.161353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa\nimport tensorflow_probability as tfp","metadata":{"execution":{"iopub.status.busy":"2022-05-30T00:50:11.043118Z","iopub.execute_input":"2022-05-30T00:50:11.043529Z","iopub.status.idle":"2022-05-30T00:50:12.522703Z","shell.execute_reply.started":"2022-05-30T00:50:11.043493Z","shell.execute_reply":"2022-05-30T00:50:12.521895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_neural_network(n_features: int, n_classes: int,\n                         n_latent: int, n_hidden: int, depth: int,\n                         nn_name: str) -> Tuple[tf.keras.Model, tf.keras.Model, \\\n                                                tf.keras.Model]:\n    if n_hidden < 1:\n        err_msg = f'The hidden layer size = {n_hidden} is too small!'\n    else:\n        info_msg = f'There are {n_features} features.'\n    print(info_msg)\n    feature_vector = tf.keras.layers.Input(\n        shape=(n_features,), dtype=tf.float32,\n        name=f'{nn_name}_feature_vector'\n    )\n    try:\n        kernel_initializer = tf.keras.initializers.LecunNormal(\n            seed=random.randint(0, 2147483647)\n        )\n    except:\n        kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n            seed=random.randint(0, 2147483647)\n        )\n    encoder_layer = tf.keras.layers.Dense(\n        units=n_hidden,\n        activation='selu',\n        kernel_initializer=kernel_initializer,\n        bias_initializer='zeros',\n        name=f'{nn_name}_enc_dense1'\n    )(feature_vector)\n    for layer_idx in range(1, depth):\n        try:\n            kernel_initializer = tf.keras.initializers.LecunNormal(\n                seed=random.randint(0, 2147483647)\n            )\n        except:\n            kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n                seed=random.randint(0, 2147483647)\n            )\n        encoder_layer = tf.keras.layers.Dense(\n            units=n_hidden,\n            activation='selu',\n            kernel_initializer=kernel_initializer,\n            bias_initializer='zeros',\n            name=f'{nn_name}_enc_dense{layer_idx + 1}'\n        )(encoder_layer)\n    prior = tfp.distributions.Independent(\n        distribution=tfp.distributions.Normal(\n            loc=tf.zeros(n_latent),\n            scale=1\n        ),\n        reinterpreted_batch_ndims=1\n    )\n    try:\n        kernel_initializer = tf.keras.initializers.LecunNormal(\n            seed=random.randint(0, 2147483647)\n        )\n    except:\n        kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n            seed=random.randint(0, 2147483647)\n        )\n    latent_layer = tf.keras.layers.Dense(\n        units=tfp.layers.IndependentNormal.params_size(n_latent),\n        activation=None,\n        kernel_initializer=kernel_initializer,\n        bias_initializer='zeros',\n        name=f'{nn_name}_latent'\n    )(encoder_layer)\n    z = tfp.layers.IndependentNormal(\n        event_shape=n_latent,\n        convert_to_tensor_fn=tfp.distributions.Distribution.sample,\n        activity_regularizer=tfp.layers.KLDivergenceRegularizer(\n            distribution_b=prior,\n            weight=1e-3\n        ),\n        name=f'{nn_name}_z'\n    )(latent_layer)\n    classifier_input = tf.keras.layers.Input(\n        shape=(n_latent,), dtype=tf.float32,\n        name=f'{nn_name}_feature_vector'\n    )\n    try:\n        kernel_initializer = tf.keras.initializers.LecunNormal(\n            seed=random.randint(0, 2147483647)\n        )\n    except:\n        kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n            seed=random.randint(0, 2147483647)\n        )\n    hidden_layer = tf.keras.layers.Dense(\n        units=n_hidden,\n        activation='selu',\n        kernel_initializer=kernel_initializer,\n        bias_initializer='zeros',\n        name=f'{nn_name}_cls_hidden'\n    )(classifier_input)\n    cls_layer = tf.keras.layers.Dense(\n        units=n_classes,\n        activation='softmax',\n        kernel_initializer=kernel_initializer,\n        bias_initializer='zeros',\n        name=f'{nn_name}_cls_output'\n    )(hidden_layer)\n    cls_name = f'{nn_name}_cls'\n    cls_model = tf.keras.Model(\n        inputs=classifier_input,\n        outputs=cls_layer,\n        name=cls_name\n    )\n    cls_model.build(input_shape=[None, n_latent])\n    try:\n        kernel_initializer = tf.keras.initializers.LecunNormal(\n            seed=random.randint(0, 2147483647)\n        )\n    except:\n        kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n            seed=random.randint(0, 2147483647)\n        )\n    decoder_layer = tf.keras.layers.Dense(\n        units=n_hidden,\n        activation='selu',\n        kernel_initializer=kernel_initializer,\n        bias_initializer='zeros',\n        name=f'{nn_name}_dec_dense1'\n    )(z)\n    for layer_idx in range(1, depth):\n        try:\n            kernel_initializer = tf.keras.initializers.LecunNormal(\n                seed=random.randint(0, 2147483647)\n            )\n        except:\n            kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n                seed=random.randint(0, 2147483647)\n            )\n        decoder_layer = tf.keras.layers.Dense(\n            units=n_hidden,\n            activation='selu',\n            kernel_initializer=kernel_initializer,\n            bias_initializer='zeros',\n            name=f'{nn_name}_dec_dense{layer_idx + 1}'\n        )(decoder_layer)\n    try:\n        kernel_initializer = tf.keras.initializers.LecunNormal(\n            seed=random.randint(0, 2147483647)\n        )\n    except:\n        kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n            seed=random.randint(0, 2147483647)\n        )\n    reconstruction_name =   f'{nn_name}_reconstruction'\n    reconstruction_layer = tf.keras.layers.Dense(\n        units=n_features,\n        activation=None,\n        kernel_initializer=kernel_initializer,\n        bias_initializer='zeros',\n        name=reconstruction_name\n    )(decoder_layer)\n    encoder_model = tf.keras.Model(\n        inputs=feature_vector,\n        outputs=z,\n        name=f'{nn_name}_enc'\n    )\n    vae_cls_model = tf.keras.Model(\n        inputs=feature_vector,\n        outputs=cls_model(z),\n        name=f'{nn_name}_vae_cls'\n    )\n    vae_name = f'{nn_name}_vae'\n    vae_model = tf.keras.Model(\n        inputs=feature_vector,\n        outputs=reconstruction_layer,\n        name=vae_name,\n    )\n    encoder_model.build(input_shape=[None, n_features])\n    vae_cls_model.build(input_shape=[None, n_features])\n    metrics = {cls_name: [tf.keras.metrics.CategoricalAccuracy()]}\n    loss_cls =  tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n    loss_vae = tf.keras.losses.LogCosh()\n    loss_weights_cls = 1.0\n    loss_weights_vae = 1.5\n    adam = tf.optimizers.Adam(learning_rate=0.01)\n    #united_model.compile(optimizer=ranger, loss=losses, loss_weights=loss_weights,\n    #                     metrics=metrics)\n    vae_model.compile(optimizer=radam, loss=loss_vae, loss_weights=loss_weights_vae)\n    return vae_model, vae_cls_model, encoder_model, cls_model","metadata":{"execution":{"iopub.status.busy":"2022-05-30T01:46:42.272869Z","iopub.execute_input":"2022-05-30T01:46:42.273234Z","iopub.status.idle":"2022-05-30T01:46:42.305515Z","shell.execute_reply.started":"2022-05-30T01:46:42.273203Z","shell.execute_reply":"2022-05-30T01:46:42.30467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_nn, vae_cls_nn, encoding_nn, classification_nn = build_neural_network(\n    n_features=concat_features.shape[1], n_classes=29,\n    n_latent=64, n_hidden=200, depth=6, nn_name='deeploma_ner'\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T01:55:15.057949Z","iopub.execute_input":"2022-05-30T01:55:15.058303Z","iopub.status.idle":"2022-05-30T01:55:15.250593Z","shell.execute_reply.started":"2022-05-30T01:55:15.058258Z","shell.execute_reply":"2022-05-30T01:55:15.249752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_nn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T01:45:56.874177Z","iopub.execute_input":"2022-05-30T01:45:56.874852Z","iopub.status.idle":"2022-05-30T01:45:56.88418Z","shell.execute_reply.started":"2022-05-30T01:45:56.874809Z","shell.execute_reply":"2022-05-30T01:45:56.883353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_nn_fname = 'deeploma_vae.h5'","metadata":{"execution":{"iopub.status.busy":"2022-05-30T00:55:37.157589Z","iopub.execute_input":"2022-05-30T00:55:37.157936Z","iopub.status.idle":"2022-05-30T00:55:37.164278Z","shell.execute_reply.started":"2022-05-30T00:55:37.157905Z","shell.execute_reply":"2022-05-30T00:55:37.163268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=vae_nn_fname,\n        monitor=\"val_loss\",\n        mode=\"min\",\n        save_best_only=True,\n        save_weights_only=True\n    ),\n]","metadata":{"execution":{"iopub.status.busy":"2022-05-30T01:38:57.832859Z","iopub.execute_input":"2022-05-30T01:38:57.833222Z","iopub.status.idle":"2022-05-30T01:38:57.83822Z","shell.execute_reply.started":"2022-05-30T01:38:57.833192Z","shell.execute_reply":"2022-05-30T01:38:57.837346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_history = vae_nn.fit(training_set, validation_data=training_set,\n                                 epochs=100, callbacks=callbacks,\n                                 verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T01:55:19.17795Z","iopub.execute_input":"2022-05-30T01:55:19.178321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model_history\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T01:36:53.942722Z","iopub.execute_input":"2022-05-30T01:36:53.943079Z","iopub.status.idle":"2022-05-30T01:36:53.967059Z","shell.execute_reply.started":"2022-05-30T01:36:53.943049Z","shell.execute_reply":"2022-05-30T01:36:53.965768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set = tf.data.Dataset.from_tensor_slices(\n    (\n        prep_data_train[0],\n        tuple(prep_data_train[1])\n    )\n.shuffle(prep_data[0].shape[0]).batch(16)\n)\n\nvalidation_set = tf.data.Dataset.from_tensor_slices(\n    (\n        prep_data_val[0],\n        tuple(prep_data_val[1])\n    )\n.shuffle(prep_data_val[0].shape[0]).batch(16)\n)","metadata":{"cellId":"dkbrmtykut9m30rartlg0s","id":"13107d61","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_cls_nn.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_cls_nn_fname = \"deeploma_vae_cls.h5\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=vae_cls_nn_fname,\n        monitor=\"val_loss\",\n        mode=\"min\",\n        save_best_only=True,\n        save_weights_only=True\n    ),\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_history = vae_cls_nn.fit(training_set, validation_data=validation_set,\n                                 epochs=100, callbacks=callbacks,\n                                 verbose=1)","metadata":{},"execution_count":null,"outputs":[]}]}